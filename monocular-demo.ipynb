{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZpoIAeweWxk"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGOlpC7OaHBN",
        "outputId": "fd935783-b785-4498-8f4a-a75744a8a893"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "%env MUJOCO_GL=egl\n",
        "\n",
        "# limit jax and TF from consuming all GPU memory\n",
        "%env XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
        "\n",
        "# Check if GPU is available\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(\"TensorFlow is using the GPU\")\n",
        "else:\n",
        "    print(\"TensorFlow is not using the GPU\")\n",
        "\n",
        "\n",
        "gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "if gpus:\n",
        "    try:\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.list_logical_devices(\"GPU\")\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)\n",
        "\n",
        "\n",
        "import jax\n",
        "jax.config.update(\"jax_enable_x64\", True)\n",
        "import jax.numpy as jnp\n",
        "\n",
        "# Check for available GPU devices\n",
        "num_devices = jax.local_device_count()\n",
        "print(f\"Found {num_devices} JAX devices:\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKkgil4DaJvE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBPlANaFcC0m"
      },
      "outputs": [],
      "source": [
        "# test the body model loads\n",
        "from monocular_demos.biomechanics_mjx.forward_kinematics import ForwardKinematics\n",
        "\n",
        "fk = ForwardKinematics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhtjaa_celBE"
      },
      "source": [
        "# Run MeTAbs-ACAE on the video\n",
        "\n",
        "First upload a video to your colab environment and then select it with the next cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaYVDlb5pH59",
        "outputId": "4a5ef7f1-10cd-4553-dbb1-2905980b033c"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "FjZULo2fZUjX",
        "outputId": "3664ef34-29c2-4724-f0fd-e46175e40e50"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# List files in current directory\n",
        "files = os.listdir()\n",
        "files = [f for f in files if 'mp4' in f or 'MOV' in f or 'MP4' in f]\n",
        "\n",
        "if len(files) > 1:\n",
        "    print(\"Available files:\")\n",
        "    for i, file in enumerate(files):\n",
        "        print(f\"{i+1}. {file}\")\n",
        "\n",
        "    # Prompt user for selection\n",
        "    choice = int(input(\"Enter the number of the file to select: \")) - 1\n",
        "    video_filepath = files[choice]\n",
        "\n",
        "    print(f\"You selected: {video_filepath}\")\n",
        "\n",
        "else:\n",
        "\n",
        "    assert len(files) == 1, \"No videos uploaded\"\n",
        "\n",
        "    video_filepath = files[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvqnF3dNaCjU"
      },
      "outputs": [],
      "source": [
        "model = hub.load('https://bit.ly/metrabs_l')  # Takes about 3 minutes\n",
        "\n",
        "# there are many skeleton formats support by this model. we are selecting one\n",
        "# compatible with the gait transformer we will use below\n",
        "skeleton = 'bml_movi_87'\n",
        "\n",
        "# get the joint names and the edges between them for visualization below\n",
        "joint_names = model.per_skeleton_joint_names[skeleton].numpy().astype(str)\n",
        "joint_edges = model.per_skeleton_joint_edges[skeleton].numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "frame_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEQooGDeacyB"
      },
      "outputs": [],
      "source": [
        "from monocular_demos.utils import video_reader\n",
        "from tqdm import tqdm\n",
        "\n",
        "vid, n_frames = video_reader(video_filepath)\n",
        "\n",
        "accumulated = None\n",
        "for i, frame_batch in tqdm(enumerate(vid)):\n",
        "    pred = model.detect_poses_batched(frame_batch, skeleton=skeleton)\n",
        "\n",
        "    if accumulated is None:\n",
        "        accumulated = pred\n",
        "\n",
        "    else:\n",
        "        # concatenate the ragged tensor along the batch for each element in the dictionary\n",
        "        for key in accumulated.keys():\n",
        "            accumulated[key] = tf.concat([accumulated[key], pred[key]], axis=0)\n",
        "\n",
        "    # if i > 10:\n",
        "    #     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPFb5gPbm-mg"
      },
      "outputs": [],
      "source": [
        "num_people = [p.shape[0] for p in accumulated['poses2d']]\n",
        "\n",
        "# assert this is 1 for all the frames\n",
        "assert len(set(num_people)) == 1\n",
        "\n",
        "# then extract the information for that person\n",
        "boxes = np.array([p[0] for p in accumulated['boxes']])\n",
        "pose3d = np.array([p[0] for p in accumulated['poses3d']])\n",
        "pose2d = np.array([p[0] for p in accumulated['poses2d']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fS3RX2mMY_dm"
      },
      "outputs": [],
      "source": [
        "# For convenience, save the keypoints in case the notebook crashes or you have to restart\n",
        "\n",
        "pose3d = np.array([p[0] for p in accumulated['poses3d']])\n",
        "\n",
        "with open('keypoints3d.npz', 'wb') as f:\n",
        "    np.savez(f, pose3d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCDNvA_G7XW5"
      },
      "source": [
        "# Exploration step: try to extract the knee angle over time\n",
        "\n",
        "Example approach: take the cross product between limb segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Pn0np4Q8JdO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_6UMGLB8KDe"
      },
      "source": [
        "# Now compute kinematics end-to-end using a differentiable body model\n",
        "\n",
        "This uses an implicit representation $f_\\theta: t \\rightarrow \\theta \\in \\mathbb R^{40}$, which is then passed through the forward kinematic model to get the predicted 3D keypoints: $\\mathcal M_\\beta: \\theta \\rightarrow \\mathbf y \\in \\mathbb R^{87 \\times 3}$.\n",
        "\n",
        "We optimize the difference between the predicted 3D keypoints and the detected 3D keypoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MikOk2_B7nWV"
      },
      "outputs": [],
      "source": [
        "with open('keypoints3d.npz', 'rb') as f:\n",
        "    pose3d = np.load(f, allow_pickle=True)['arr_0']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pose3d = np.array([p[0] for p in accumulated['poses3d']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nf_AVjhVk5Tp"
      },
      "outputs": [],
      "source": [
        "from jax import numpy as jnp\n",
        "\n",
        "# convert pose to m\n",
        "pose = pose3d\n",
        "pose = pose[:, :, [0, 2, 1]]\n",
        "pose[:, :, 2] *= -1\n",
        "pose /= 1000.0\n",
        "\n",
        "pose = pose - np.min(pose, axis=1, keepdims=True)\n",
        "\n",
        "timestamps = jnp.arange(len(pose)) / 30.0\n",
        "\n",
        "dataset = (timestamps, pose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGSE5_IEoavG"
      },
      "outputs": [],
      "source": [
        "from jaxtyping import Integer, Float, Array, PRNGKeyArray\n",
        "from typing import Tuple, Dict\n",
        "from tqdm import trange\n",
        "import equinox as eqx\n",
        "import optax\n",
        "\n",
        "from monocular_demos.biomechanics_mjx.monocular_trajectory import KineticsWrapper, get_default_wrapper\n",
        "\n",
        "# construct a loss function between the forward pass through the forward kinematic\n",
        "# implicit representation and the resulting keypoint and the detected keypoitns\n",
        "\n",
        "def loss(\n",
        "    model: KineticsWrapper,\n",
        "    x: Float[Array, \"times\"],\n",
        "    y: Float[Array, \"times keypoints 3\"],\n",
        "    site_offset_regularization = 1e-1\n",
        ") -> Tuple[Float, Dict]:\n",
        "\n",
        "    timestamps = x\n",
        "    keypoints3d = y\n",
        "    metrics = {}\n",
        "\n",
        "    # NOTE: steps is an make sure this retraces for different dimensions\n",
        "    (state, constraints, next_states), (ang, vel, action), _ = model(\n",
        "        timestamps,\n",
        "        skip_vel=True,\n",
        "        skip_action=True,\n",
        "    )\n",
        "\n",
        "    pred_kp3d = state.site_xpos\n",
        "\n",
        "    l = jnp.mean((pred_kp3d - keypoints3d) ** 2) * 100 # so in cm\n",
        "    metrics[\"kp_err\"] = l\n",
        "\n",
        "    # regularize marker offset\n",
        "    l_site_offset = jnp.sum(jnp.square(model.site_offsets))\n",
        "    l += l_site_offset * site_offset_regularization\n",
        "\n",
        "    # make loss the first key in the dictionary by popping and building a new dictionary with the rest\n",
        "    metrics = {\"loss\": l, **metrics}\n",
        "\n",
        "    return l, metrics\n",
        "\n",
        "\n",
        "@eqx.filter_jit\n",
        "def step(model, opt_state, data, loss_grad, optimizer, **kwargs):\n",
        "    x, targets = data\n",
        "\n",
        "    (val, metrics), grads = loss_grad(model, x=x, y=targets, **kwargs)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state, model)\n",
        "    model = eqx.apply_updates(model, updates)\n",
        "    return val, model, opt_state, metrics\n",
        "\n",
        "\n",
        "def fit_model(\n",
        "    model: KineticsWrapper,\n",
        "    dataset: Tuple,\n",
        "    lr_end_value: float = 1e-8,\n",
        "    lr_init_value: float = 1e-4,\n",
        "    max_iters: int = 5000,\n",
        "    clip_by_global_norm: float = 0.1,\n",
        "):\n",
        "\n",
        "    # work out the transition steps to make the desired schedule\n",
        "    transition_steps = 10\n",
        "    lr_decay_rate = (lr_end_value / lr_init_value) ** (1.0 / (max_iters // transition_steps))\n",
        "    learning_rate = optax.warmup_exponential_decay_schedule(\n",
        "        init_value=0,\n",
        "        warmup_steps=0,\n",
        "        peak_value=lr_init_value,\n",
        "        end_value=lr_end_value,\n",
        "        decay_rate=lr_decay_rate,\n",
        "        transition_steps=transition_steps,\n",
        "    )\n",
        "\n",
        "    optimizer = optax.chain(\n",
        "        optax.adamw(learning_rate=learning_rate, b1=0.8, weight_decay=1e-5), optax.zero_nans(), optax.clip_by_global_norm(clip_by_global_norm)\n",
        "    )\n",
        "    opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
        "\n",
        "    loss_grad = eqx.filter_value_and_grad(loss, has_aux=True)\n",
        "\n",
        "    counter = trange(max_iters)\n",
        "    for i in counter:\n",
        "\n",
        "        val, model, opt_state, metrics = step(model, opt_state, dataset, loss_grad, optimizer)\n",
        "\n",
        "        if i > 0 and i % int(max_iters // 10) == 0:\n",
        "            print(f\"iter: {i} loss: {val}.\")  # metrics: {metrics}\")\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            metrics = {k: v.item() for k,v in metrics.items()}\n",
        "            print(val, metrics)\n",
        "\n",
        "    return model, metrics\n",
        "\n",
        "\n",
        "fkw = get_default_wrapper()\n",
        "updated_model, metrics = fit_model(fkw, dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMQJ_t3X85zm"
      },
      "source": [
        "# Now explore the results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpVUeeWwF9Az"
      },
      "outputs": [],
      "source": [
        "(state, constraints, next_states), (ang, vel, action), _ = updated_model(dataset[0], skip_vel=True, skip_action=True)\n",
        "jnp.mean((state.site_xpos.shape - dataset[1]) ** 2)\n",
        "\n",
        "# plot the knees\n",
        "plt.figure()\n",
        "plt.plot(ang[:, [9, 16]]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8bkECg8zSZw"
      },
      "outputs": [],
      "source": [
        "from body_models.biomechanics_mjx.visualize import render_trajectory, jupyter_embed_video\n",
        "\n",
        "fn = 'reconstruction.mp4'\n",
        "render_trajectory(ang, fn, xml_path=None)\n",
        "HTML = jupyter_embed_video(fn)\n",
        "HTML"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "IZpoIAeweWxk"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "irisight-api",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
